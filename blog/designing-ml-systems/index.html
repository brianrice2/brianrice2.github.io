<!doctype html>

<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="author" content="Brian Rice" />
    <meta name="    <meta name=" viewport" content="width=device-width" />
    <title> Designing Machine Learning Systems </title>
    <link rel="canonical" href="https://brianrice2.github.io/blog/designing-ml-systems/" />
    <link rel="stylesheet" type="text/css" href="/main.css" />
     
    <link rel="icon" href="/favicon.ico" />
</head>

<body>
    <nav>
        <a href="/">Home</a><a href="/about">About</a><a href="/blog">Blog</a><a href="/recipes">Recipes</a>
    </nav>
    <h1> Designing Machine Learning Systems </h1>
     
    </header>
    <main> <p><a href="https://huyenchip.com">Chip Huyen</a> recently released an excellent book, <a href="https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969">Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications</a>. Recently I also read this book and recommend it wholeheartedly to anyone interested in the end-to-end machine learning lifecycle (go buy it now!). As I read through the book, I kept some notes of each chapter to improve my recall of the contents. I can’t pretend that these notes cover all the topics addressed in the book, but here they are :)</p>
<h2 id="chapter-notes">Chapter Notes</h2>
<h3 id="1-overview-of-machine-learning-systems">1. Overview of Machine Learning Systems</h3>
<p>Machine learning solutions are appropriate when you want to (1) <em>learn</em> (2) <em>complex patterns</em> from (3) <em>existing data</em> and use these patterns to make (4) <em>predictions</em> on (5) <em>unseen data</em>. ML solutions are particularly effective when (6) the task is repetitive, (7) the cost of wrong predictions is cheap, and (8) it’s at scale.</p>
<h3 id="2-introduction-to-machine-learning-systems-design">2. Introduction to Machine Learning Systems Design</h3>
<p>Different teams often have different priorities, and it can be difficult to weigh competing metrics (it’s much easier to optimize for a single metric). At the end of the day, profit is the primary driver, so it’s critical to tie an ML system’s performance to the overall business performance.</p>
<p>ML systems should meet four characteristics: <em>reliability</em> (performing even in the face of faults), <em>scalability</em> (efficiently adjusting to traffic volume), <em>maintainability</em> (relevant software, data, and artifacts are high-quality, documented, tracked, and organized), and <em>adaptability</em> (the system can respond to data and business changes without downtime)</p>
<p>In real life, developing these ML systems is very much an iterative process, where the interaction between steps is continuously reexamined:</p>
<ol>
<li>Project scoping</li>
<li>Data engineering</li>
<li>ML model development</li>
<li>Deployment</li>
<li>Monitoring and continual learning</li>
<li>Business analysis</li>
</ol>
<h3 id="3-data-engineering-fundamentals">3. Data Engineering Fundamentals</h3>
<p>Common data formats can be either text (human-readable) or binary, and row-major or column-major. In general, binary data is more performant and compact but cannot be directly inspected. <em>Row-major</em> formats like CSV offer easy writes and appends; <em>column-major</em> formats like Parquet offer great (column-based) read performance, especially useful for analytical workloads.</p>
<p><em>Data models</em> describe how data is represented. Two common models are relational and NoSQL (of which document and graph models are notable subtypes).</p>
<p><em>Transaction processing</em> must be done quickly and be highly available; on the other hand, <em>analytical processing</em> can support higher latency and often aggregate across multiple data sources.</p>
<p>Data movement can take the form of data passing through databases, through services (request-driven and synchronous), and through real-time transport (event-driven, asynchronous, and communicate through brokers, e.g. pubsub and message queues).</p>
<h3 id="4-training-data">4. Training Data</h3>
<p>Data in the real world can be hard to collect and severely imbalanced, and <em>sampling</em> and <em>data augmentation</em> help to curate the right datasets to build ML on top of. <em>Natural labels</em> are collected or created automatically by the system and are very useful for ML applications like recommender systems, although the labels may take a while to be created and stored.</p>
<h3 id="5-feature-engineering">5. Feature Engineering</h3>
<p>Missing values can be of three types:</p>
<ul>
<li><em>Missing not at random</em> (MNAR) means the reason a value is missing is because of the true value itself. For example, some people may choose not to respond when asked their income because it is especially low or high.</li>
<li><em>Missing at random</em> (MAR) means another observed variable influences the fact that the value is missing. For example, people of one gender may prefer to not disclose their age.</li>
<li><em>Missing completely at random</em> (MCAR) means there is really no pattern to when the value is missing. This type is very rare. Deletion is most appropriate for this type of missing value as long as the number of affected rows is small.</li>
</ul>
<p><em>Data leakage</em> is a particularly sneaky reason why ML applications fail in production. Pay careful attention to how you split time-correlated data; handle scaling, missing values, and duplicate values when splitting; and leakage that may come from the data generation process itself.</p>
<p>Continually revising the features included for a model is imperative for managing technical debt—too many features can result in overfitting, data leakage, increased latency/storage/memory, and unused features (aka tech debt). Use features that are important to the model and that generalize well, and remove features that are no longer useful even if they once were.</p>
<h3 id="6-model-development-and-offline-evaluation">6. Model Development and Offline Evaluation</h3>
<p>When deciding which model is a good fit for your application, consider:</p>
<ul>
<li>Data, compute, and training time requirements</li>
<li>Inference latency</li>
<li>Interpretability</li>
</ul>
<p>Start with the simplest models and add complexity only when absolutely necessary! Consider that some models may perform worse now but be easier to update later (for example, a neural network that can continually learn vs a collaborative filtering model that cannot). Shifting from one model to an ensemble of models will probably be much harder to deploy, maintain, and debug, but the extra performance could potentially yield massive gains.</p>
<p>Tracking and versioning your experiments is very helpful for reproducibility, but can feel tedious—use tooling and infrastructure to ease the burden. Distributed training can make large models and training workloads feasible; AutoML can also help to tune parameters or test a massive number of different model architectures.</p>
<p>Having a good baseline to compare a challenger model against is essential for gauging relative performance—this point of reference could be random, a simple heuristic, the majority clas, human performance, or an existing (champion) model. For better robustness in production, employ perturbation, invariance, and directional expectation tests. Measure the model’s calibration, confidence measurement, and performance on different slices.</p>
<h3 id="7-model-deployment-and-prediction-service">7. Model Deployment and Prediction Service</h3>
<p>ML engineers should expect to have to maintain many models, that those models’ performance will degrade and will need frequent updating, and to eventually have to scale up.</p>
<p>Batch prediction uses only batch features, but online prediction can use any combination of batch and/or streaming features. Unifying batch and streaming pipelines goes a long way to ensuring consistency and preventing nasty bugs.</p>
<p>Models can be compressed through methods like <em>low-rank factorization</em> (replacing high-dimensional tensors with low-dimensional ones), <em>knowledge distillation</em> (a teacher model trains a smaller student model), <em>pruning</em> (removing nodes or setting parameters to 0), and <em>quantization</em> (reducing precision by using fewer bits to represent parameters).</p>
<p>Deploying models to the cloud is easier but more expensive; deploying to <em>edge devices</em> is more hardware-constrained but provides less latency, better availability in poor connectivity areas, and more user privacy.</p>
<ul>
<li>Compiling and optimizing models for edge devices is challenging! <em>Intermediate representations (IRs)</em> are a series of progressively lower-level representations that translate high-level (e.g., PyTorch) code into machine code. Optimization engineers are usually brought in to optimize models for a certain hardware, although ML is getting increasingly involved in this step.</li>
</ul>
<h3 id="8-data-distribution-shifts-and-monitoring">8. Data Distribution Shifts and Monitoring</h3>
<p>ML systems fail because of all the same reasons that software systems fail <em>and then some</em>: dependencies, deployment, hardware, etc. But differences in data between development and production, as well as new edge cases and degenerate feedback loops (where a system’s outputs influence its future inputs), raise the bar further.</p>
<p>Common types of data distribution shifts are:</p>
<ul>
<li><em>Covariate shift</em>, where the input distribution changes but the prediction model P(Y;X) does not change</li>
<li><em>Label shift</em>, where the output distribution changes but, for a given output, the input distribution stays the same. This is related to covariate shift and often shows up together.</li>
<li><em>Concept drift</em>, where the same input yields a different output. This is often cyclic or seasonal.</li>
</ul>
<p>Monitoring ML systems can be useful at different points of the stack: from raw inputs to features to predictions to performance metrics. Raw inputs are harder to monitor than performance metrics but are less likely to be caused by human errors; on the other hand, performance metrics are closest to the business metrics that the system is there to help.</p>
<h3 id="9-continual-learning-and-test-in-production">9. Continual Learning and Test in Production</h3>
<h4 id="continual-learning">Continual Learning</h4>
<p><em>Continual learning</em> allows us to respond to data distribution changes by continually updating ML models. It represents a paradigm shift from the traditional approach of batch learning, but opens many doors to better performance. The “learning” in “continual learning” may be done statelessly (retraining models from scratch with newer data) or statefully (continuing the training of a previous model with newer data)—similar to transfer learning and fine-tuning, the latter requires much less data. Both of these are examples of <em>data iteration</em>, where the same model is refreshed with different data, as opposed to <em>model iteration</em>, where the features or architectures of a model are changed.</p>
<h4 id="testing-in-production">Testing in Production</h4>
<p>Many problems encountered in machine learning model deployment are unique to a production environment; they cannot be (reasonably) recreated in a test environment. So there are some strategies to make the rollout of a new model safer and with higher confidence that it will improve the business metric for which it exists:</p>
<ul>
<li><em>Shadow deployment</em> involves deploying a challenger model in the background and logging its predictions to be analyzed later (while still returning predictions from the champion model). If the challenger’s performance looks okay, it can then be promoted.</li>
<li><em>A/B testing</em> randomly assigns groups of users to interact with models. Commonly, two models A and B are compared with each other, but the experiment may be set up for more than one treatment (i.e., A/B/C/D testing). Since this is a statistical experiment, it’s important to ensure that user assignment is completely random, that there are no confounding variables (like which type of device users are on), and that groups and predictions are isolated from one another.</li>
<li><em>Canary release</em> or <em>canary deployment</em> rolls out the challenger model to a small subset of users, devices, etc. If the performance meets or exceeds that of the champion model, its rollout progresses; if the performance plummets, its rollout is immediately rolled back and the champion model remains in effect.</li>
<li><em>Interleaved experiments</em> display predictions from both models side-by-side and is used especially in the context of recommender systems. Deciding if model A or model B is better becomes much easier when recommendations from both models are displayed and the user can act on their preferred item. However, the information provided relates mostly to users’ preferences, not <strong>XYZ</strong>.</li>
<li><em>Bandits</em> are the most data-efficient option, but also the most complex to implement—for this reason, their use is most common in large tech companies.</li>
</ul>
<h3 id="10-infrastructure-and-tooling-for-mlops">10. Infrastructure and Tooling for MLOps</h3>
<p>The infrastructure used for powering machine learning at scale can be split into four layers:</p>
<ol>
<li><em>Storage and compute</em> allow for collection and storage of data, and provide the resources necessary to run ML and other workloads</li>
<li><em>Resource management</em> schedules and orchestrates workloads to make better use of your compute resources</li>
<li><em>ML platform</em> provides ML-specific tooling to help with development and deployment of ML applications, like model registries, feature stores, and monitoring tools</li>
<li><em>Development environments</em> are where code is written and experiments are run</li>
</ol>
<h4 id="storage-and-compute">Storage and Compute</h4>
<p>Data storage can be handled in a variety of ways, but it’s so cheap now that many companies default to storing everything in the cloud. This topic is covered mostly in the previous chapters.</p>
<p>Compute may be ephemeral (think serverless like AWS Step Functions or Google Cloud Run) or more long-lived (like EC2).</p>
<h4 id="resource-management">Resource Management</h4>
<p>Schedulers deal with <em>when</em> jobs run while orchestrators deal with <em>where</em> jobs run, but there is often some overlap between frameworks. Workflow management tools are similar to schedulers, but deal more with tasks and workflows as a whole rather than individual jobs (think high-level vs. low-level).</p>
<p>Cron is a simple job scheduler. More complex solutions like Airflow can handle dependencies between jobs as well as resource management.</p>
<ul>
<li>Airflow</li>
<li>Prefect</li>
<li>Argo</li>
<li>Kubeflow</li>
<li>Metaflow</li>
</ul>
<h4 id="ml-platform">ML Platform</h4>
<ul>
<li><em>Model deployment</em> services should make it easy for you to offer online and batch predictions (with batch predictions being a better indicator of quality, as it involves the nuances of job scheduling and prediction storage) as well as performing different test-in-production schemes described in chapter 9</li>
<li><em>Model stores</em> help track ML artifacts like: model definitions and parameters, software and data dependencies, model generation code, other experiment artifacts, and metadata tags (like model owners or the relevant business task). Even <a href="https://mlflow.org">MLflow</a>, the most popular model store, still is far from solving the artifact problem.</li>
<li><em>Feature stores</em> help address feature management (sharing, discovering, and describing features), transformation (performing computation and storing the results for reuse), and/or consistency (ensure aligment between features for dev/production environments, batch/streaming pipelines, and training/inference).</li>
</ul>
<h4 id="development-environments">Development Environments</h4>
<p>Dev environments are where data scientists and engineers spend most of their time. Improvements you make here can vastly improve the productivity for many users. Progressing down the dependency spectrum of standardized packages to standardized Python/language versions to standardized dev containers to cloud-based development environments can greatly improve the consistency and experience across teams as well as reduce the burden of supporting many different IDEs and environment configurations.</p>
<h3 id="11-the-human-side-of-machine-learning">11. The Human Side of Machine Learning</h3>
<p>It can be common to have a separate team managing production and deployments, but this often yields worse communication/collaboration and more difficult debugging (worse visibility). Having end-to-end data scientists is harder to recruit and train for, but can have great impact.</p>
 </main>
</body>

</html>